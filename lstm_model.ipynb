{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOH+5vzWBRFxz2gk5zzoYEx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlwodnr59/-LSTM-model-for-binary-classification/blob/main/lstm_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GeddiER5PpV2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def resize_images(root_path, output_path, size=(224,224)):\n",
        "    for dirpath, _, filenames in os.walk(root_path):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.jpg'):\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                with Image.open(filepath) as img:\n",
        "                    img = img.resize(size, Image.BICUBIC)\n",
        "                    img.save(os.path.join(output_path, filename))\n",
        "\n",
        "def create_dataset(root_path):\n",
        "    data = []\n",
        "    for dirpath, _, filenames in os.walk(root_path):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith('.npy'):\n",
        "                label = int(dirpath[-1])\n",
        "                filepath = os.path.join(dirpath, filename)\n",
        "                with open(filepath, 'rb') as f:\n",
        "                    img = np.load(f)\n",
        "                    data.append((img, label))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence, label = self.data[idx]\n",
        "        return torch.from_numpy(sequence), label\n",
        "\n",
        "def create_dataloader(data, batch_size=32):\n",
        "    dataset = SequenceDataset(data)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "89oiPOFJR3WZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "def train_model(model, dataloader, optimizer, criterion, device, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f'Epoch {epoch+1} training loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "id": "xXLutCcMR4B6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# 이미지 크기를 맞춰주는 함수\n",
        "def resize_image(image_path, size=(64, 64)):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, size)\n",
        "    return img.astype(np.float32) / 255.0\n",
        "\n",
        "# 데이터셋 경로와 시퀸스 길이를 지정합니다.\n",
        "data_path = 'new'\n",
        "sequence_length = 20\n",
        "\n",
        "\n",
        "\n",
        "def load_data(data_path, sequence_length):\n",
        "    X = []\n",
        "    y = []\n",
        "    for label in range(2):\n",
        "        img_path = f'{data_path}/{label}/img'\n",
        "        if label == 0:\n",
        "          for i in range(900, 1093 - sequence_length + 1):\n",
        "            img_seq = []\n",
        "            for j in range(i, i+sequence_length):\n",
        "                img = resize_image(f'{img_path}/img{j}.jpg', size=(64, 64))\n",
        "                img_seq.append(img)\n",
        "            X.append(img_seq)\n",
        "            y.append(label)\n",
        "        else:\n",
        "          for i in range(1093, 1148 - sequence_length + 1):\n",
        "            img_seq = []\n",
        "            for j in range(i, i+sequence_length):\n",
        "                img = resize_image(f'{img_path}/img{j}.jpg', size=(64, 64))\n",
        "                img_seq.append(img)\n",
        "            X.append(img_seq)\n",
        "            y.append(label)\n",
        "        \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 데이터셋 로드\n",
        "X, y = load_data(data_path, sequence_length)\n",
        "\n",
        "# 시퀸스 길이 축을 추가하여 X를 reshape 합니다.\n",
        "X = X.reshape((-1, sequence_length, 64, 64, 3))\n",
        "print(X.shape)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "# LSTM 모델 정의\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(sequence_length, 64, 64, 3)),\n",
        "    layers.TimeDistributed(layers.Conv2D(32, 3, activation='relu')),\n",
        "    layers.TimeDistributed(layers.MaxPooling2D(2)),\n",
        "    layers.TimeDistributed(layers.Flatten()),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 모델 컴파일 및 학습\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=30, batch_size=32, validation_split=0.3,callbacks=[early_stop])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwDZXlImSnze",
        "outputId": "6c803d10-3aab-4c44-d8c9-850ce2efb106"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 20, 64, 64, 3)\n",
            "Epoch 1/30\n",
            "5/5 [==============================] - 22s 4s/step - loss: 0.1895 - accuracy: 0.7823 - val_loss: 2.8545 - val_accuracy: 0.4286\n",
            "Epoch 2/30\n",
            "5/5 [==============================] - 21s 4s/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.0143 - val_accuracy: 0.4286\n",
            "Epoch 3/30\n",
            "5/5 [==============================] - 17s 4s/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 3.1635 - val_accuracy: 0.4286\n",
            "Epoch 4/30\n",
            "5/5 [==============================] - 18s 4s/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 3.2285 - val_accuracy: 0.4286\n",
            "Epoch 5/30\n",
            "5/5 [==============================] - 17s 4s/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.2966 - val_accuracy: 0.4286\n",
            "Epoch 6/30\n",
            "5/5 [==============================] - 18s 4s/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 3.3446 - val_accuracy: 0.4286\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f274012dca0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install CustomDataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qumKmB9sS2Ys",
        "outputId": "66647c9a-337c-4e0b-8b5e-6c88b50b7e21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement CustomDataset (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for CustomDataset\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ]
}